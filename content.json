[{"title":"爬虫概述","date":"2017-03-21T11:14:18.000Z","path":"2017/03/21/爬虫/","text":"Requests库的使用 首先，request是第三方库，这里用非常方便的install requests``` 来进行安装。1234567891011121314&lt;!--more--&gt;##### 发送请求 在写程序之前，我们要先引用requests库，代码如``import requests`` ，这样就可以使用requests库了。requests库有如下方法：get , post , put , delete , head , options 。在实际编写爬虫代码中，最长用到的就是使用get方法来获取某个页面，这里举例获取百度首页，代码如下：``response = requests.get(&apos;https://www.baidu.com&apos;)``返回的response即为百度首页的源代码。##### 传递参数 requests实际代替了我们用浏览器来发送请求，但是对于服务器来说它是可以鉴别发送请求的主体是什么的，我们用Chrome来进行抓包，F12打开Network，刷新请求，我们在Request Headers 中可以看到 如下：&gt; User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36此即为服务器看到请求的主体，对于一些网站出于安全考虑来讲会限制爬虫，一些较为低级的限制方式就是识别头部，如果是浏览器即可访问，请他类似于Request 或Urllib 等则无法访问。那现在让我给Requests传递参数吧，我们构建headers 这个参数，这是一个字典，里面可以放入如 User-Agent，Cookies等参数，例```headers = &#123;‘User-Agent’：‘Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36’&#125;response = requests.get(&apos;https://www.baidu.com&apos; , headers = headers)` 这样，我们在用Requests来访问百度，服务器看到的请求就是有Mozilla/5.0 这个浏览器来发送的，而不是爬虫。","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"hello world","date":"2017-03-19T02:24:18.000Z","path":"2017/03/19/hello-world/","text":"Hello world","tags":[{"name":"我的第一篇博文","slug":"我的第一篇博文","permalink":"http://yoursite.com/tags/我的第一篇博文/"}]}]