[{"title":"爬虫概述","date":"2017-03-21T11:14:18.000Z","path":"2017/03/21/爬虫/","text":"Requests库的使用 首先，requests是一个第三方库，大家可以看requests的官方文档，requests的使用非常友好，我们来看一下requests的开发哲学： Beautiful is better than ugly.(美丽优于丑陋)Explicit is better than implicit.(直白优于含蓄)Simple is better than complex.(简单优于复杂)Complex is better than complicated.(复杂优于繁琐)Readability counts.(可读性很重要) 所以大家只要把官方文档搞清楚，就可以熟练运用这个可以让HTTP为人类使用的库了。 好了，让我们开始一起探索吧。首先这里用非常方便的pip install requests 来进行安装。 发送请求 在写程序之前，我们要先引用requests库，代码如import requests ，这样就可以使用requests库了。requests库有如下方法：get , post , put , delete , head , options 。在实际编写爬虫代码中，最常用到的就是使用get方法来获取某个页面，这里举例获取百度首页，代码如下：response = requests.get(&#39;https://www.baidu.com&#39;)返回的response即为百度首页的源代码。 传递参数 requests实际代替了我们用浏览器来发送请求，但是对于服务器来说它是可以鉴别发送请求的主体是什么的，我们用Chrome来进行抓包，F12打开Network，刷新请求，我们在Request Headers 中可以看到 如下： User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36 此即为服务器看到请求的主体，对于一些网站出于安全考虑来讲会限制爬虫，一些较为低级的限制方式就是识别头部，如果是浏览器即可访问，其他类似于Request 或Urllib 等则无法访问。那现在让我给Requests传递参数吧，我们构建headers 这个参数，这是一个字典，里面可以放入如 User-Agent，Cookies等参数，例1234headers = &#123;‘User-Agent’：‘Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36’&#125;response = requests.get(&apos;https://www.baidu.com&apos; , headers = headers) 这样，我们在用Requests来访问百度，服务器看到的请求就是有Mozilla/5.0 这个浏览器来发送的，而不是爬虫。","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"hello world","date":"2017-03-19T02:24:18.000Z","path":"2017/03/19/hello-world/","text":"Hello world","tags":[{"name":"我的第一篇博文","slug":"我的第一篇博文","permalink":"http://yoursite.com/tags/我的第一篇博文/"}]}]